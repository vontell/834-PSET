{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Unstructured Environments\n",
    "\n",
    "1. [Introduction and Motivation](#1.-Introduction-and-Motivation)\n",
    "2. [General Techniques](#2.-General-Techniques)\n",
    "3. [Finding Interest Point Operators](#3.-Finding-Interest-Point-Operators)\n",
    "  1. [Thresholding](#a.-Thresholding)\n",
    "  2. [Picking out Features of Interest](#b.-Picking-out-Features-of-Interest)\n",
    "4. [Traditional Image Classification](#4.-Traditional-Image-Classification)\n",
    "5. [Bag of Features: Processing Training Data](#5.-Bag-of-Features:-Processing-Training-Data)\n",
    "6. [Bag of Features: Classifying New Images](#6.-Bag-of-Features:-Classifying-New-Images)\n",
    "7. [Improving Feature Detection](#7.-Improving-Feature-Detection)\n",
    "  2. [Saliency](#a.-Saliency)\n",
    "  3. [Hue Information](#b.-Hue-Information)\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "In this mini problem set, you will how to apply image classification to marine environments. Unlike typical object recognition tasks that recognize objects that have limited variability in form, content, and shape, marine image classification requires a system that can handle incredible variability in object form. Take the following images of sand and seaweed, for example:\n",
    "\n",
    "<img src=\"pictures/header.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "These images include a tangle of objects, variable colors, occlusion, and other issues that make typical object recognition inaccurate. Throughout this mini problem set, you will learn how to build better models and methods for dealing with unstructured environments such as those found in marine habitats. The goal is to create a system with a higher accuracy in recognizing parts of a marine habitat, which would come in handy during AUV surveying and planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. General Techniques\n",
    "\n",
    "Before diving into applying the new techniques learned from lecture, let's explore state-of-the-art general techniques in image recognition. A prime example of this is provided from Google's Vision API. Visit their website at [https://cloud.google.com/vision/](https://cloud.google.com/vision/), and take a look at the services provided. Let's take a look what happens when we attempt to classify two different images.\n",
    "\n",
    "First, save the image below of a horse to your computer, and upload it to the site above in the section labeled \"Try the API\". Note the words that it finds that are representative of the objects in that image.\n",
    "\n",
    "<img src=\"pictures/horse.jpg\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "Now, try doing the same thing with the below image of a seafloor. Once again, note the words that it decides are representative of the objects in the image.\n",
    "\n",
    "<img src=\"pictures/sandy_image.png\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "Given these results, answer the following questions:\n",
    "\n",
    "**a) What words does it find for each of these images? What do you notice about the type of words that it finds for each image? Why do you think this is happening?**\n",
    "\n",
    "**b) Why do you think it would be unfair to compare these two images using Google's system?**\n",
    "<br/>\n",
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Possible answers for horse image:\n",
    "Horse, Jockey, Rein, Horse Like Mammal, Horse Racing, Stallion, Mare, Horse Track, Equestrian...\n",
    "\n",
    "Possible answers for coral image:\n",
    "Texture, Geology, Soil, Rock, Concrete\n",
    "\n",
    "Google's answers are more specific for the horse image than for the coral one.\n",
    "\n",
    "b) Answers may vary, but something along the lines of the answers below suffice:\n",
    "A. It is possible that Google did not offer options for coral during the training process; therefore it was not one of the possible labels.\n",
    "B. The images of coral that Google used to train looked nothing like the image that we provided (which was more similar to images of rocks and soil).\n",
    "C. Google optimized their algorithm for the \"common case\" (people are more likely to ask for images about horses than underwater images of coral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finding Interest Point Operators\n",
    "\n",
    "Interest point operators can help us find points or features in the image that are unique and tell us something about the picture as a whole. These features are often found on the edges and corners of the image because these boundary points separate out the foreground, where interesting features are, from the lighter image background. We will be exploring a technique called MSER in this mini problem set to find high contrast regions of the image which represent edges, and sample features from those points.\n",
    "\n",
    "### a. Thresholding\n",
    "\n",
    "We will explore the concept of thresholding, which is the idea that a threshold is set between 0 and 255 (the values that a pixel can be) and all the pixels above that number are white and all those below the number are black. In the code below, the function MSER takes in a threshold value. Then the display image (a picture of a horse) is read as a numpy array with dimensions 128x128 with pixel values between 0 and 255. Your task is to fill in the code in the area specified so that pixel values below the threshold are 0, which means they are black and pixel values above the threshold are 255, which means they are white. Your final image should be called 'final' in order to be properly displayed. You can check yourself by ensuring that your pictures at threshold levels 80, 100 and 120 look like the following pictures respectively.\n",
    "\n",
    "<img src=\"pictures/80\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"pictures/100\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"pictures/120\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSER(threshold):\n",
    "    '''\n",
    "    threshold: the pixel value such that all pixels below this value should be black and all pixels above this value \n",
    "    should be white\n",
    "    \n",
    "    returns the image where the threshold has been applied as a numpy array\n",
    "    '''\n",
    "    display_img = cv2.imread('pictures/00000019.jpg', 0)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    black = np.asarray([[255 for _ in range(128)] for _ in range(128)])\n",
    "    higher = display_img < threshold\n",
    "    higher = higher.astype(int)\n",
    "    final = higher * black\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    plt.imshow(final, cmap='Greys'),plt.show()\n",
    "    return final\n",
    "\n",
    "#You can change the threshold to whatever you want for testing but make sure \n",
    "#to change it back to 80 before you move on to the next part\n",
    "final = MSER(80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MSER(MSER)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Picking out Features of Interest\n",
    "\n",
    "Now that we have the outline of the feature of interest, we want to find interesting points at the boundaries. We have implemented the code to find intensity changes in the horizontal x direction but you will need to add the code to find intensity changes in the vertical y direction and plot those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnPoints(picture):\n",
    "    '''\n",
    "    picture: 128x128 numpy array that has been thresholded\n",
    "    \n",
    "    return a set containing points where the gradient changes in the y direction as well as the x direction\n",
    "    '''\n",
    "    display_x_img = cv2.imread('pictures/00000019.jpg', 0)\n",
    "    points = set()\n",
    "    \n",
    "    #code to find intensity changes in the horizontal x direction\n",
    "    for x in range(len(picture)):\n",
    "        for y in range(1, len(picture[0])):\n",
    "            if abs(picture[x][y] - picture[x][y-1]) > 254:\n",
    "                points.add((x, y))\n",
    "                points.add((x, y-1))\n",
    "                \n",
    "    ### BEGIN SOLUTION\n",
    "    for y in range(128):\n",
    "        for x in range(1, 128):\n",
    "            if abs(picture[x][y] - picture[x-1][y]) > 254:\n",
    "                points.add((x, y))\n",
    "                points.add((x-1, y))\n",
    "    ### END SOLUTION\n",
    "\n",
    "    for i in points:\n",
    "        y,x = i\n",
    "        cv2.circle(display_x_img,(x,y),1,255,-1)\n",
    "    \n",
    "    plt.imshow(display_x_img),plt.show()\n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "returnPoints(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returnPoints(returnPoints)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Traditional Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function for you to use to obtain the training data. This function iterates through every image file in the directory and SIFT runs on each image creating an array of feature vectors for each image. \n",
    "\n",
    "In the parent directory we have provided you with 2 folders containing images of auditoriums (labeled \"Auditorium\") and bowling alleys (labeled \"Bowling\"). \n",
    "\n",
    "The return_labels function takes in 5 inputs: \n",
    "\n",
    "* directoryList: list of directory labels\n",
    "* range_size: the size of the term vector for each image, typically (added_limit * 128) if feature vector is 128\n",
    "* added_limit: Given that not all images will not have the same number of interest points, we provide a smaller uniform number to consider. \n",
    "* count_limit: Desired number of interest points found in a single picture\n",
    "* patch_size: The size of a patch(size of a feature around an interest point in pixels) defaulted to 32\n",
    "\n",
    "\n",
    "Run the following cell to obtain the training data. You will find that return_labels does not work for high added_limits because  SIFT will not find enough feature points on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_limit = 12\n",
    "range_size =  added_limit*128\n",
    "sift_pictures = return_labels(['Bowling', 'Auditorium'], range_size, added_limit, 100)\n",
    "print(sift_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the dimensions of the output array from return_labels while considering the input arguments and explain the format of the output that it provides. You may modify the code in the cell above to print the dimensions you are interested in. \n",
    "How would you split the data to process each feature independently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please answer in the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something along the lines of the following would suffice:\n",
    "\n",
    "`return_labels` returns a Numpy array of dimension (42, range_size). The number of rows is constant because one row corresponds to the SIFT features (concatenated together) of either a 'Bowling' or 'Auditorium' photo; the amounts of both types of photos are constant throughout this exercise, so the number of rows will never change. \n",
    "\n",
    "The number of columns is equal to range_size, and each column corresponds to an entry in a SIFT descriptor. Changing `added_limit` affects the column size because that changes how many SIFT features are in each row of the result. One would be able to process each feature independently by using Numpy functions to split-out sets of 128-length vectors that appear in each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have identified values to run return_labels and SIFT, what are the best values for `range_size` and `added_limit` to get the most accurate classification results? Run the following cell to find the clusters using K-Means clustering and answer the previous question and the one below in the designated cell. (Note that the code will throw an error if `added_limit` exceeds 18). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_limit = 12\n",
    "range_size =  added_limit*128\n",
    "labels_list = ['Bowling', 'Auditorium']\n",
    "sift_pictures = return_labels(labels_list, range_size, added_limit, 100)\n",
    "print(len(sift_pictures[0]))\n",
    "kmeans = KMeans(n_clusters=2, random_state=1).fit(sift_pictures)\n",
    "print(sift_pictures)\n",
    "preds = kmeans.predict(sift_pictures)\n",
    "print(preds)\n",
    "print(len(preds))\n",
    "print(kmeans_accuracy(preds, labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do the values you found end up being the best values to run SIFT? How can you increase the accuracy? Comment on the accuracies you see, is this a good image classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please answer in the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers may vary. We found that 18 works best. One can increase accuracy by increasing added_limit, which corresponds to the number of interest points returned (so more points => better features => better classifiers). However, this is (arguably) not a good image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bag of Features: Processing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we performed image classification by considering the entire picture as a single vector, including all of the features. Now we will implement Bag Of Features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to get a set of all of the features from all of the pictures to build a vocabulary. \n",
    "Implement the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_set(sift_pictures):\n",
    "    '''\n",
    "    sift_pictures - data structure studied above that includes all features from all pictures\n",
    "    \n",
    "    return set of 128 length features from all pictures\n",
    "    \n",
    "    '''\n",
    "    # separate sift_pictures into individual pictures. \n",
    "    # for each picture, separate its descriptor into individual features (remember features are length 128).\n",
    "    # add the individual features to the output set.\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    out = []\n",
    "    for i in range(len(sift_pictures)):\n",
    "        curr_pic = sift_pictures[i]\n",
    "        parts = np.hsplit(curr_pic, curr_pic.size/128)\n",
    "        out.append(parts)\n",
    "    return np.vstack(out)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will use the feature set to build the vocabulary. HINT: use the K-Means functions from the sklearn library. You may find it useful to reference the code in section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(set_of_features, k):\n",
    "    '''\n",
    "    set_of_features - set of 128 length sift descriptors\n",
    "    k - number of terms in our vocabulary\n",
    "    \n",
    "    return an sklearn k-means classifier fitted to the feature set\n",
    "    '''\n",
    "    # use sklearn to create a k-means classifier that groups input into k clusters\n",
    "    # fit the classifier to the set of features\n",
    "    # output the classifier \n",
    "    \n",
    "    clf = KMeans(n_clusters=k, random_state=1) # don't change this line!\n",
    "    #INSERT CODE HERE\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    clf.fit(set_of_features)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return clf\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will create a term vector for one of the training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_term_vector(sift_picture, k, vocab_classifier):\n",
    "    '''\n",
    "    sift_picture - a single picture represented as a concatenation of all of its features\n",
    "    k - number of terms in our vocabulary\n",
    "    vocab_classifier - sklearn k-means classifier of vocab features\n",
    "    \n",
    "    return term vector - a list of feature counts from the input picture\n",
    "    '''\n",
    "    # intialize the term vector to be a length-k list of 0s, where k is the number of terms in the vocab\n",
    "    # separate picture into individual features\n",
    "    # for each feature:\n",
    "    #     use the kmeans classifier to classify the feature as one of the vocabulary items\n",
    "    #     increment the appropriate entry in the term vector each time a feature is found\n",
    "    # return the feature vector\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    tv = [0 for i in range(k)]\n",
    "    features = np.hsplit(sift_picture, sift_picture.size/128)\n",
    "    for feature in features:\n",
    "        result = vocab_classifier.predict(feature.reshape(1,-1))\n",
    "        tv[int(result)] += 1\n",
    "    return tv\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will put all of these helper methods together to process the training data. We will give you an outline of how to do this, but feel free to implement it however you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_training_data(added_limit, k):\n",
    "    '''\n",
    "    added_limit: Desired number of interest points found in a single picture\n",
    "    k - number of terms in our vocabulary\n",
    "    \n",
    "    returns (labeled_term_vectors, k, vocab_classifier)\n",
    "    \n",
    "            labeled_term_vectors: \n",
    "                list of tuples describing labels of known points; \n",
    "                   first element of each tuple is the term vector\n",
    "                   second element of each tuple is the label\n",
    "                   ex. [([...], 'label1'), ([...],'label2'),...]\n",
    "            vocab_classifier: sklearn k-means classifier of vocab features\n",
    "    '''\n",
    "    \n",
    "    range_size =  added_limit*128\n",
    "    labels_list = ['Bowling', 'Auditorium']\n",
    "    sift_pictures = return_labels(labels_list, range_size, added_limit, 100)\n",
    "    \n",
    "    # get the sift_pictures for the bowling alleys and auditoriums from return_labels\n",
    "    # extract the feature set from all of the pictures\n",
    "    # build a vocabulary from the feature set, get a k-means classifier for feature vectors\n",
    "    \n",
    "    # get the picture descriptions (from return_labels) for the bowling alleys alone from return_labels\n",
    "    # for each picture:\n",
    "    #     get its term vector\n",
    "    #     add it to labeled_term_vectors with the label 'Bowling'\n",
    "    \n",
    "    # get the picture descriptions (from return_labels) for the auditoriums alone from return_labels\n",
    "    # for each picture:\n",
    "    #     get its term vector\n",
    "    #     add it to labeled_term_vectors with the label 'Auditorium'\n",
    "    \n",
    "    # return (labeled_term_vectors, vocab_classifier)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # extract the feature set from all of the pictures\n",
    "    # build a vocabulary from the feature set, get a k-means classifier for feature vectors\n",
    "    features = extract_feature_set(sift_pictures)\n",
    "    clf = build_vocab(features, k)\n",
    "    labeled_term_vectors = []\n",
    "    # get the sift_pictures for the bowling alleys alone from return_labels\n",
    "    bowling_pictures = return_labels(['Bowling'], range_size, added_limit, 100)\n",
    "    # for each picture:\n",
    "    #     get its term vector\n",
    "    #     add it to labeled_term_vectors with the label 'Bowling'\n",
    "    \n",
    "    for pic in bowling_pictures:\n",
    "        labeled_term_vectors.append((generate_term_vector(pic, k, clf), 'Bowling'))\n",
    "    \n",
    "    # get the sift_pictures for the auditoriums alone from return_labels\n",
    "    auditorium_pictures = return_labels(['Auditorium'], range_size, added_limit, 100)\n",
    "    # for each picture:\n",
    "    #     get its term vector\n",
    "    #     add it to labeled_term_vectors with the label 'Auditorium'\n",
    "    for pic in auditorium_pictures:\n",
    "        labeled_term_vectors.append((generate_term_vector(pic, k, clf), 'Auditorium'))\n",
    "    \n",
    "    return (labeled_term_vectors, clf)\n",
    "\n",
    "    ### END SOLUTION\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_process_training_data(process_training_data)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Bag of Features: Classifying New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having processed the training images, building a vocabulary and getting term vectors, we now want to be able to classify an unknown image. We can use the same functions as we developed in the previous section to convert a new image to a term vector, but then we need to figure out the classification of the new term vector.\n",
    "\n",
    "To do this, we will use the k-nearest-neighbors (KNN) algorithm. Given a set of labeled data (in this case term vectors corresponding to images) as \"examples\", the algorithm will classify a new datum based on the $k$ examples to which the unobserved datum is \"nearest\".\n",
    "\n",
    "There are many ways to define distance between one example and another, but one of the most straightforward and common approaches is to use Euclidean distance (or \"L2-norm\" if you want to be fancy).\n",
    "\n",
    "For this next part of the assignment, implement the KNN algorithm in the space provided. Then you will use this to classify some unknown images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_closest(point, known_points, k):\n",
    "    '''\n",
    "    point - unknown point (Numpy array of term vectors)\n",
    "    \n",
    "    known_points - list of tuples describing labels of known points; \n",
    "                   first element of each tuple is the sift feature vector\n",
    "                   second element of each tuple is the corresponding label\n",
    "                   ex. [([...], 'label1'), ([...],'label2'),...]\n",
    "                   \n",
    "    k - an integer (corresponding to the number of neighbors to consider)\n",
    "    \n",
    "    Returns: the k tuples from known_points corresponding to the k closest points\n",
    "             to the unknown point (as a list)\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    distances = np.zeros(len(known_points))\n",
    "    for i in range(len(known_points)):\n",
    "        dist = np.linalg.norm(np.array(point) - np.array(known_points[i][0]))\n",
    "        distances[i] = dist\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    results = []\n",
    "    for i in range(len(indices)):\n",
    "        results.append(known_points[indices[i]])\n",
    "    return results\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(known_points, unknown_points, k):\n",
    "    '''\n",
    "    known_points - list of tuples describing labels of known points; \n",
    "                   first element of each tuple is the sift feature vector\n",
    "                   second element of each tuple is the corresponding label\n",
    "                   ex. [([...], 'label1'), ([...],'label2'),...]\n",
    "                   \n",
    "    unknown_points - list of term vectors (Numpy arrays) corresponding to \n",
    "                    examples needing classification\n",
    "                    \n",
    "    k - an integer (corresponding to the number of neighbors to consider)\n",
    "    \n",
    "    Returns: list of tuples describing the assigned labels of previously unknown points\n",
    "             (in the same order as the input)\n",
    "    '''\n",
    "    ### BEGIN SOLUTIONS\n",
    "    results = []\n",
    "    for i in range(len(unknown_points)):\n",
    "        neighbors = get_k_closest(unknown_points[i], known_points, k)\n",
    "        freqs = {}\n",
    "        for neighbor in neighbors:\n",
    "            label = neighbor[1]\n",
    "            if label not in freqs:\n",
    "                freqs[label] = 0\n",
    "            freqs[label] += 1\n",
    "        \n",
    "        max_label = -1\n",
    "        max_amt = -1\n",
    "        for label in freqs:\n",
    "            if freqs[label] > max_amt:\n",
    "                max_amt = freqs[label]\n",
    "                max_label = label\n",
    "        results.append((unknown_points[i], max_label))\n",
    "    return results\n",
    "    ### END SOLUTIONS\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer the following questions about your KNN implementation in the cell below.\n",
    "\n",
    "1. Why don't we want to use even numbers as values of $k$?\n",
    "2. How would you go about finding the best value of $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solutions for 1 and 2:\n",
    "\n",
    "1. By using an even number for $k$, it becomes possible to run into situations where there is no clear majority class for an unknown point when comparing neighbors. This is why $k$ should always be an odd number.\n",
    "\n",
    "2. Answers may vary. Anything mentioning a split in the data to test different values of $k$ (such as training-validation, train-test, train-val-test, cross-validation, etc.) should receive credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use this implementation to classify new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_new_images(added_limit, k, neighbors):\n",
    "    '''\n",
    "    k: parameter defining the number of terms in the vocabulary\n",
    "    neighbors: number of neighbors to consider when classifying\n",
    "    \n",
    "    return list of tuples describing the assigned labels of previously unknown images from the \"unknown\" directory\n",
    "    '''\n",
    "    range_size =  added_limit*128\n",
    "    known_pictures = return_labels(['Bowling', 'Auditorium'], range_size, added_limit, 100)\n",
    "    unknown_pictures = return_labels(['Unknown'], range_size, added_limit, 100)\n",
    "    \n",
    "    # process the training pictures to get a labeled list of term vectors and a vocab classifier\n",
    "    #    (use the function you implemented in 4)\n",
    "    \n",
    "    # create a list of term vectors of the unknown images:\n",
    "    # for each of the pictures in the unknown picture directory:\n",
    "    #     generate a term vector\n",
    "    #     add the term vector to the list of term vectors\n",
    "    \n",
    "    # use your knn implementation to label the list of unknown term vectors\n",
    "    \n",
    "    # return the labeled images\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    labeled_term_vectors, clf = process_training_data(added_limit, k)\n",
    "    unknown_term_vectors = []\n",
    "    for pic in unknown_pictures:\n",
    "        vec = generate_term_vector(pic, k, clf)\n",
    "        unknown_term_vectors.append(vec)\n",
    "    return knn(labeled_term_vectors, unknown_term_vectors, neighbors)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_knn(classify_new_images)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your bag of features implementation in the tester below. You can change the parameters k, added_limit, and neighbors to maximize your accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "added_limit = 18\n",
    "neighbors = 3\n",
    "\n",
    "labeled_images = classify_new_images(k, added_limit, neighbors)\n",
    "print \"Accuracy: \",  str(check_bof_accuracy(labeled_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Improving Feature Detection\n",
    "\n",
    "Now that we have gone through the process of obtaining feature vectors for our training images and classifying test images against this data, we can begin to make adjustments for handling unstructured environments. As covered in the lecture, two useful extensions are the use of saliency and hue information.\n",
    "\n",
    "### a. Saliency\n",
    "Recall from the lecture that saliency is a measure of entropy used to determine how \"interesting\" a patch is. We can use this to select regions of images that are better for training. Recall that the entropy for a discrete random variable is given by\n",
    "\n",
    "$$E(\\Delta I)=H=-\\sum_{i=1}^{n}p(x_{i})\\log p(x_{i})$$\n",
    "\n",
    "For the gray scale image we define the probability $p(x)$ as follows,\n",
    "\n",
    "$$p(x_{i})={h(x_{i\\over N})={1\\over N}\\sum_{j=1}^{N}\\delta(x_{i}-x_{j})}$$\n",
    "\n",
    "where $\\delta$ is the discrete Dirac delta function:\n",
    "$$\\delta(x)=\\cases{1, x=0\\cr 0, x\\neq 0 } $$\n",
    "\n",
    "So finally the entropy of a patch with $N_p$ pixels will be:\n",
    "$$H_{g}=-{1\\over N_{p}}\\sum_{j=1}^{N_{p}}\\log p_{g}(x_{j}) $$\n",
    "It is important to note that we are calculating the log probability, $p_{g}$ using all the pixels in the image, but the outermost sum is just over the pixels in the patch.\n",
    "\n",
    "Implement the function below which returns the total entropy for a given patch. The parameter `log_probs` is a list of values which represents the log probability of each pixel based off the global distribution of pixels.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Implement the method below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_entropy(log_probs):\n",
    "    '''\n",
    "    log_probs - list of values which represents the log probability of each pixel\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    return -1*sum(log_probs)/len(log_probs)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_patch_entropy():\n",
    "    test_patch_entropy_solution(get_patch_entropy)\n",
    "    test_ok()\n",
    "test_get_patch_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Hue Information\n",
    "As explained in the introduction, color is an important feature in correctly classifying unstructured environments such as coral. In order to incorporate this data into our training / features, we add the average hue of the patch to the feature vectors that are created using SIFT. The hue value can be visualized below, and is found by convering a pixel in RGB into the HSV color space.\n",
    "\n",
    "<img src=\"pictures/hue.png\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "If we have a 128-dimension feature vector from SIFT representing a patch, then the new feature vector would be of dimension 129. However, a trick/hack that we can employ is to copy the hue value into the feature vector multiple times. By adding, say, 10 more features which each hold the hue data, we may be able to achieve higher classification accuracies.\n",
    "\n",
    "The graph below shows the change in classification accuracy as we change the number of times the hue feature has been added to the patch feature vector. The classification accuracy was found by classifying seaweed against sand.\n",
    "\n",
    "<img src=\"pictures/hue_graph.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Given these results, answer the following questions:\n",
    "\n",
    "**a) What do you notice about the effect of hue data on classifiction accuracy? Do you notice anything strange, or anything that was expected?**\n",
    "\n",
    "**b) Why do you think that hue was a good feature to include, especially with these data sets?**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solutions below:\n",
    "\n",
    "a) As the number of times hue is included as a feature increases, the classification accuracy increases. Students might expect the function to be linear, exponential, quadratic, etc. but something *not* stepwise.\n",
    "\n",
    "b) Sufficient answers include anything mentioning huge contrast between images of seaweed and images of sand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Congratulations, you've completed this problem set! Please save this notebook, validate it, and submit it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
