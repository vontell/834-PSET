{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from siftdetector import detect_keypoints\n",
    "%matplotlib inline\n",
    "from numpy_sift import SIFTDescriptor \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/dev/opencv-3.1/build/lib')\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from math import log\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finding Interest Point Operators\n",
    "\n",
    "Interest point operators can help us find points or features in the image that are unique and tell us something about the picture as a whole. These features are often found on the edges and corners of the image because these boundary points separate out the foreground, where interesting features are, from the lighter image background. We will be exploring a technique called MSER in this mini problem set to find high contrast regions of the image which represent edges, and sample features from those points.\n",
    "\n",
    "## Thresholding\n",
    "\n",
    "We will explore the concept of thresholding, which is the idea that a threshold is set between 0 and 255 (the values that a pixel can be) and all the pixels above that number are white and all those below the number are black. In the code below, the function MSER takes in a threshold value. Then the display image (a picture of a horse) is read as a numpy array with dimensions 128x128 with pixel values between 0 and 255. Your task is to fill in the code in the area specified so that pixel values below the threshold are 0, which means they are black and pixel values above the threshold are 255, which means they are white. Your final image should be called 'final' in order to be properly displayed. You can check yourself by ensuring that your pictures at threshold levels 80, 100 and 120 look like the following pictures respectively.\n",
    "\n",
    "<img src=\"80\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"100\" width=\"200\" height=\"200\"/>\n",
    "<img src=\"120\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSER(threshold):\n",
    "    display_img = cv2.imread('00000019.jpg', 0)\n",
    "    #Fill in code here \n",
    "\n",
    "    \n",
    "    black = np.asarray([[255 for _ in range(128)] for _ in range(128)])\n",
    "    higher = display_img < threshold\n",
    "    higher = higher.astype(int)\n",
    "    final = higher * black\n",
    "    \n",
    "    \n",
    "    plt.imshow(final, cmap='Greys'),plt.show()\n",
    "    return final\n",
    "\n",
    "#You can change the threshold to whatever you want for testing but make sure \n",
    "#to change it back to 80 before you move on to the next part\n",
    "final = MSER(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking out Features of Interest\n",
    "\n",
    "Now that we have the outline of the feature of interest, we want to find interesting points at the boundaries. We have implemented the code to find intensity changes in the horizontal x direction but you will need to add the code to find intensity changes in the vertical y direction and plot those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnPoints(picture):\n",
    "    display_x_img = cv2.imread('00000019.jpg', 0)\n",
    "    points = set()\n",
    "    \n",
    "    #code to find intensity changes in the horizontal x direction\n",
    "    for x in range(len(picture)):\n",
    "        for y in range(1, len(picture[0])):\n",
    "            if abs(picture[x][y] - picture[x][y-1]) > 254:\n",
    "                points.add((x, y))\n",
    "\n",
    "    \n",
    "    #Add your code here to add the (x,y) coordinates of points that represent an intensity change \n",
    "    #in the y direction. The points should be added to the set points.\n",
    "\n",
    "    \n",
    "    for i in points:\n",
    "        y,x = i\n",
    "        cv2.circle(display_x_img,(x,y),1,255,-1)\n",
    "    \n",
    "    plt.imshow(display_x_img),plt.show()\n",
    "\n",
    "\n",
    "\n",
    "returnPoints(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_img = cv2.imread('reject/00000002.jpg', 0)\n",
    "black = np.asarray([[255 for _ in range(128)] for _ in range(128)])\n",
    "print display_img[0][127]\n",
    "print display_img[127][0]\n",
    "def thresholding(threshold):\n",
    "    higher = display_img < threshold\n",
    "    print display_img\n",
    "    print higher\n",
    "    print higher.astype(int)\n",
    "    higher = higher.astype(int)\n",
    "    final = higher * black\n",
    "    print \"final\", final\n",
    "    plt.imshow(final, cmap='Greys'),plt.show()\n",
    "    return final\n",
    "    \n",
    "final = thresholding(100)\n",
    "\n",
    "def find_point(picture, value):\n",
    "    points = []\n",
    "    for x in range(len(picture)):\n",
    "        for y in range(1, len(picture[0])):\n",
    "            if abs(picture[x][y] - picture[x][y-1]) > value:\n",
    "                points.append((x, y))\n",
    "    return points\n",
    "\n",
    "display_points = find_point(final, 256)\n",
    "\n",
    "print display_points\n",
    "#display_points = [(0, 127)]\n",
    "for i in display_points:\n",
    "    y,x = i\n",
    "    cv2.circle(display_img,(x,y),1,255,-1)\n",
    "    \n",
    "plt.imshow(display_img),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnPointsXdirection(picture):\n",
    "    display_x_img = cv2.imread('00000019.jpg', 0)\n",
    "    points = set()\n",
    "    for x in range(len(picture)):\n",
    "        for y in range(1, len(picture[0])):\n",
    "            if abs(picture[x][y] - picture[x][y-1]) > 254:\n",
    "                points.add((x, y))\n",
    "\n",
    "    #add your code here\n",
    "    for y in range(128):\n",
    "        for x in range(1, 128):\n",
    "            if abs(picture[x][y] - picture[x-1][y]) > 254:\n",
    "                points.add((x, y))\n",
    "\n",
    "    \n",
    "    for i in points:\n",
    "        y,x = i\n",
    "        cv2.circle(display_x_img,(x,y),1,255,-1)\n",
    "    \n",
    "    plt.imshow(display_x_img),plt.show()\n",
    "\n",
    "\n",
    "\n",
    "returnPointsXdirection(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSER(threshold):\n",
    "    display_img = cv2.imread('00000019.jpg', 0)\n",
    "    #Fill in code here \n",
    "\n",
    "    \n",
    "    black = np.asarray([[255 for _ in range(128)] for _ in range(128)])\n",
    "    higher = display_img < threshold\n",
    "    higher = higher.astype(int)\n",
    "    final = higher * black\n",
    "    \n",
    "    \n",
    "    plt.imshow(final, cmap='Greys'),plt.show()\n",
    "    return final\n",
    "\n",
    "#You can change the threshold to whatever you want for testing but make sure \n",
    "#to change it back to 80 before you move on to the next part\n",
    "final = MSER(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_labels(directoryList, range_size, added_limit, count_limit, patch_size=32):\n",
    "    SD = SIFTDescriptor(patchSize = patch_size)\n",
    "\n",
    "    #creates initial vector of size 3968\n",
    "    sift_pictures = np.asarray([[0 for _ in range(range_size)]])\n",
    "    for directory in directoryList:\n",
    "        #go through the directories with the relevant images\n",
    "        for filename in os.listdir(directory):\n",
    "            name = directory + '/' + filename\n",
    "            print name\n",
    "            if name[-3:] == 'png' or name[-3:] == 'jpg':\n",
    "                image = cv2.imread(name)\n",
    "                \n",
    "                #transform it to black and white and get features\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                h, w = gray.shape\n",
    "                corners = cv2.goodFeaturesToTrack(gray,count_limit,0.001,10)\n",
    "\n",
    "                corners = np.int0(corners)\n",
    "                image_sift_features = np.asarray([]) \n",
    "                count, actually_added = 0, 0\n",
    "                \n",
    "                #Here we want to get 31 images for our feature vector and then break out of the loop\n",
    "                while actually_added < added_limit and count < count_limit:\n",
    "                    corner = corners[count][0]\n",
    "                    if patch_size/2 <= corner[1] <= h-patch_size/2 and patch_size/2 <= corner[0] <= w-patch_size/2:\n",
    "                        patch = gray[corner[1]-patch_size/2:corner[1]+patch_size/2, corner[0]-patch_size/2:corner[0]+patch_size/2]\n",
    "                        sift = SD.describe(patch)\n",
    "                        image_sift_features = np.append(image_sift_features, sift)\n",
    "                        actually_added += 1\n",
    "                    count += 1\n",
    "\n",
    "                sift_pictures = np.append(sift_pictures, [image_sift_features], axis=0)\n",
    "\n",
    "    return sift_pictures[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auditorium/.DS_Store\n",
      "Auditorium/00000001.jpg\n",
      "Auditorium/00000003.jpg\n",
      "Auditorium/00000004.jpg\n",
      "Auditorium/00000008.jpg\n",
      "Auditorium/00000009.jpg\n",
      "Auditorium/00000011.jpg\n",
      "Auditorium/00000016.jpg\n",
      "Auditorium/00000022.jpg\n",
      "Auditorium/00000024.jpg\n",
      "Auditorium/00000025.jpg\n",
      "Auditorium/00000026.jpg\n",
      "Auditorium/00000030.jpg\n",
      "Auditorium/00000031.jpg\n",
      "Auditorium/00000032.jpg\n",
      "Auditorium/00000049.jpg\n",
      "Auditorium/00000062.jpg\n",
      "Bowling/.DS_Store\n",
      "Bowling/00000006.jpg\n",
      "Bowling/00000010(1).jpg\n",
      "Bowling/00000011.jpg\n",
      "Bowling/00000012(1).jpg\n",
      "Bowling/00000026(1).jpg\n",
      "Bowling/00000029(1).jpg\n",
      "Bowling/00000031.jpg\n",
      "Bowling/00000044.jpg\n",
      "Bowling/00000055(1).jpg\n",
      "Bowling/00000058.jpg\n",
      "Bowling/00000059.jpg\n",
      "Bowling/00000072.jpg\n",
      "Bowling/00000095.jpg\n",
      "Bowling/00000098.jpg\n",
      "Bowling/00000104.jpg\n",
      "Bowling/00000126(1).jpg\n",
      "Bowling/00000129(1).jpg\n",
      "Bowling/00000134(1).jpg\n",
      "test/.DS_Store\n",
      "test/00000002.jpg\n",
      "test/00000017.jpg\n",
      "test/00000162.jpg\n",
      "test/00000190.jpg\n",
      "test/00000510.jpg\n",
      "test/00000591.jpg\n",
      "[1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect number of features. Got 3968 features, expected 2304",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-33ebe77c5c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_pictures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3968\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msift_pictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cluster_centers_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0mx_squared_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_labels_inertia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc\u001b[0m in \u001b[0;36m_check_test_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    869\u001b[0m             raise ValueError(\"Incorrect number of features. \"\n\u001b[1;32m    870\u001b[0m                              \"Got %d features, expected %d\" % (\n\u001b[0;32m--> 871\u001b[0;31m                                  n_features, expected_n_features))\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incorrect number of features. Got 3968 features, expected 2304"
     ]
    }
   ],
   "source": [
    "sift_pictures = return_labels(['Auditorium', 'Bowling'], 2304, 18, 100)\n",
    "kmeans = KMeans(n_clusters=2, random_state=1).fit(sift_pictures)\n",
    "test_pictures = return_labels(['test'], 3968, 31, 100)\n",
    "print kmeans.predict(sift_pictures)\n",
    "print kmeans.predict(test_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
