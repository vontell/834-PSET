{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Unstructured Environments\n",
    "\n",
    "1. [Introduction and Motivation](#1.-Introduction-and-Motivation)\n",
    "2. [General Techniques](#2.-General-Techniques)\n",
    "3. Building a vocabulary\n",
    "  1. Obtain training data\n",
    "  2. Get SIFT feature vectors for all training images\n",
    "  3. K-means clustering to get term vectors\n",
    "4. Classifying images\n",
    "  1. Load some test data, and test it\n",
    "5. [Improving Feature Detection](#5.-Improving-Feature-Detection)\n",
    "  2. [Saliency](#a.-Saliency)\n",
    "  3. [Hue Information](#b.-Hue-Information)\n",
    "6. Conclusion\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "In this mini problem set, you will how to apply image classification to marine environments. Unlike typical object recognition tasks that recognize objects that have limited variability in form, content, and shape, marine image classification requires a system that can handle incredible variability in object form. Take the following images of sand and seaweed, for example:\n",
    "\n",
    "<img src=\"header.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "These images include a tangle of objects, variable colors, occlusion, and other issues that make typical object recognition inaccurate. Throughout this mini problem set, you will learn how to build better models and methods for dealing with unstructured environments such as those found in marine habitats. The goal is to create a system with a higher accuracy in recognizing parts of a marine habitat, which would come in handy during AUV surveying and planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. General Techniques\n",
    "\n",
    "Before diving into applying the new techniques learned from lecture, let's explore state-of-the-art general techniques in image recognition. A prime example of this is provided from Google's Vision API. Visit their website at [https://cloud.google.com/vision/](https://cloud.google.com/vision/), and take a look at the services provided. Let's take a look what happens when we attempt to classify two different images.\n",
    "\n",
    "First, save the image below of a horse to your computer, and upload it to the site above in the section labeled \"Try the API\". Note the words that it finds that are representative of the objects in that image.\n",
    "\n",
    "<img src=\"horse.jpg\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "Now, try doing the same thing with the below image of a seafloor. Once again, note the words that it decides are representative of the objects in the image.\n",
    "\n",
    "<img src=\"sandy_image.png\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "Given these results, answer the following questions:\n",
    "\n",
    "**a) What words does it find for each of these images? What do you notice about the type of words that it finds for each image? Why do you think this is happening?**\n",
    "\n",
    "**b) Why do you think it would be unfair to compare these two images using Google's system?**\n",
    "<br/>\n",
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building A Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function for you to use to obtain the training data for the Clustering Algorithm. This function iterates through every image file in the directory and SIFT runs on each image creating an array of feature vectors for each image. \n",
    "\n",
    "In the parent directory we have provided you with 2 folders containing images of auditoriums (labeled \"Auditorium\") and bowling alleys (labeled \"Bowling\"). \n",
    "\n",
    "The return_labels function takes in 5 inputs: \n",
    "\n",
    "* directoryList: list of directory labels\n",
    "* range_size: the size of the term vector for each image, typically (added_limt * 128) if feature vector is 128\n",
    "* added_limit: Given that not all images will not have the same number of interest points, we provide a smaller uniform number to consider. \n",
    "* count_limit: Desired number of interest points found in a single picture\n",
    "* patch_size: The size of a patch(size of a feature around an interest point in pixels) defaulted to 32\n",
    "\n",
    "\n",
    "Run the following cell to obtain the training data. You will need to find an appropriate count_limit and range_size. Remember range_size is (128*count_limit). You will find that return_labels does not work for high count_limits because  SIFT will not find enough feature points on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "('range: ', 2304)\n",
      "('count: ', 100)\n",
      "[[  6.   8.   2. ...   4.  18.  73.]\n",
      " [ 21.  12.  23. ...  30.  23.   6.]\n",
      " [ 14.  15.  44. ...   3.   2.   1.]\n",
      " ...\n",
      " [  4.   5.   6. ...  84. 101.  40.]\n",
      " [  8.   8.   7. ...  24.  29.  30.]\n",
      " [ 24.  27.  33. ...  38.  28.  29.]]\n"
     ]
    }
   ],
   "source": [
    "sift_pictures = return_labels(['Bowling', 'Auditorium'], 2304, 18, 100)\n",
    "print(sift_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the dimensions of the output array from return_labels while considering the input arguments and explain the format of the output that it provides?\n",
    "How would you split the data to process each feature independently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please answer in the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have identified values to run return_labels and SIFT, what is the best values for range_size and count_limit to get the most accurate classification results. Run the following cell to find the clusters using K-Means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "('range: ', 2304)\n",
      "('count: ', 100)\n",
      "[[   0.    1.    1. ...,   14.   20.   19.]\n",
      " [  30.   29.   18. ...,    3.    1.    0.]\n",
      " [  39.  123.   51. ...,   50.   80.   53.]\n",
      " ..., \n",
      " [  16.   29.   17. ...,    9.    1.    0.]\n",
      " [  68.   48.    1. ...,    1.    5.    8.]\n",
      " [   3.    3.    3. ...,   18.   29.   19.]]\n",
      "[1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 1]\n",
      "0.530612244898\n"
     ]
    }
   ],
   "source": [
    "count_limit = 18\n",
    "range_size =  count_limit*128\n",
    "labels_list = ['Bowling', 'Auditorium']\n",
    "sift_pictures = return_labels(labels_list, range_size, count_limit, 100)\n",
    "kmeans = KMeans(n_clusters=2, random_state=1).fit(sift_pictures)\n",
    "print(sift_pictures)\n",
    "preds = kmeans.predict(sift_pictures)\n",
    "print(preds)\n",
    "\n",
    "print(kmeans_accuracy(preds, labels_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do the values you found end up being the best values to run SIFT? How can you increase the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please answer in the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classifying Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the features that you found in the previous part, let's now implement a simple (yet powerful) classification algorithm.\n",
    "\n",
    "Recall from lecture the K-nearest-neighbors (KNN) algorithm. Given a set of labeled data (in this case SIFT features corresponding to images) as \"examples\", the algorithm will classify a new datum based on the $k$ examples to which the unobserved datum is \"nearest\".\n",
    "\n",
    "There are many ways to define distance between one example and another, but one of the most straightforward and common approaches is to use Euclidean distance (or \"L2-norm\" if you want to be fancy).\n",
    "\n",
    "For this next part of the assignment, implement the KNN algorithm in the space provided. Then proceed to respond to the short-answer questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_closest(point, known_points, k):\n",
    "    '''\n",
    "    point - unknown point (Numpy array of sift features)\n",
    "    \n",
    "    known_points - list of tuples describing labels of known points; \n",
    "                   first element of each tuple is the sift feature vector\n",
    "                   second element of each tuple is the corresponding label\n",
    "                   ex. [([...], 'label1'), ([...],'label2'),...]\n",
    "                   \n",
    "    k - an integer (corresponding to the number of neighbors to consider)\n",
    "    \n",
    "    Returns: the k tuples from known_points corresponding to the k closest points\n",
    "             to the unknown point (as a list)\n",
    "    '''\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(known_points, unknown_points, k):\n",
    "    '''\n",
    "    known_points - list of tuples describing labels of known points; \n",
    "                   first element of each tuple is the sift feature vector\n",
    "                   second element of each tuple is the corresponding label\n",
    "                   ex. [([...], 'label1'), ([...],'label2'),...]\n",
    "                   \n",
    "    unknown_points - list of sift feature vectors (Numpy arrays) corresponding to \n",
    "                    examples needing classification\n",
    "                    \n",
    "    k - an integer (corresponding to the number of neighbors to consider)\n",
    "    \n",
    "    Returns: list of tuples describing the assigned labels of previously unknown points\n",
    "             (in the same order as the input)\n",
    "    '''\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer the following questions about your implementation in the cell below.\n",
    "\n",
    "1. Why don't we want to use even numbers as values of $k$?\n",
    "2. How would you go about finding the best value of $k$?\n",
    "3. What was the best value of $k$ that worked for the data given above? (TODO: figure out data to give)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improving Feature Detection\n",
    "\n",
    "Now that we have gone through the process of obtaining feature vectors for our training images and classifying test images against this data, we can begin to make adjustments for handling unstructured environments. As covered in the lecture, two useful extensions are the use of saliency and hue information.\n",
    "\n",
    "### a. Saliency\n",
    "Recall from the lecture that saliency is a measure of entropy used to determine how \"interesting\" a patch is. We can use this to select regions of images that are better for training. Recall that the entropy for a discrete random variable is given by\n",
    "\n",
    "$$E(\\Delta I)=H=-\\sum_{i=1}^{n}p(x_{i})\\log p(x_{i})$$\n",
    "\n",
    "For the gray scale image we define the probability $p(x)$ as follows,\n",
    "\n",
    "$$p(x_{i})={h(x_{i\\over N})={1\\over N}\\sum_{j=1}^{N}\\delta(x_{i}-x_{j})}$$\n",
    "\n",
    "where $\\delta$ is the discrete Dirac delta function:\n",
    "$$\\delta(x)=\\cases{1, x=0\\cr 0, x\\neq 0 } $$\n",
    "\n",
    "So finally the entropy of a patch with $N_p$ pixels will be:\n",
    "$$H_{g}=-{1\\over N_{p}}\\sum_{j=1}^{N_{p}}\\log p_{g}(x_{j}) $$\n",
    "It is important to note that we are calculating the log probability, $p_{g}$ using all the pixels in the image, but the outermost sum is just over the pixels in the patch.\n",
    "\n",
    "Implement the function below which returns the total entropy for a given patch. The parameter `log_probs` is a list of values which represents the log probability of each pixel based off the global distribution of pixels.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Implement the method below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_entropy(log_probs):\n",
    "    '''\n",
    "    log_probs - list of values which represents the log probability of each pixel\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6790894a61b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_patch_entropy_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_patch_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_get_patch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6790894a61b6>\u001b[0m in \u001b[0;36mtest_get_patch_entropy\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_get_patch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_patch_entropy_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_patch_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_get_patch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/834-PSET/utils.py\u001b[0m in \u001b[0;36mtest_patch_entropy_solution\u001b[0;34m(get_patch_entropy)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_patch_entropy_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_patch_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtest_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mget_patch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_probs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.3888888888888889\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Total patch entropy is incorrect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-100f3db7b8e1>\u001b[0m in \u001b[0;36mget_patch_entropy\u001b[0;34m(log_probs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_get_patch_entropy():\n",
    "    test_patch_entropy_solution(get_patch_entropy)\n",
    "    test_ok()\n",
    "test_get_patch_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Hue Information\n",
    "As explained in the introduction, color is an important feature in correctly classifying unstructured environments such as coral. In order to incorporate this data into our training / features, we add the average hue of the patch to the feature vectors that are created using SIFT. The hue value can be visualized below, and is found by convering a pixel in RGB into the HSV color space.\n",
    "\n",
    "<img src=\"hue.png\" width=\"200\" height=\"200\"/>\n",
    "\n",
    "If we have a 128-dimension feature vector from SIFT representing a patch, then the new feature vector would be of dimension 129. However, a trick/hack that we can employ is to copy the hue value into the feature vector multiple times. By adding, say, 10 more features which each hold the hue data, we may be able to achieve higher classification accuracies.\n",
    "\n",
    "The graph below shows the change in classification accuracy as we change the number of times the hue feature has been added to the patch feature vector. The classification accuracy was found by classifying seaweed against sand.\n",
    "\n",
    "<img src=\"hue_graph.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "Given these results, answer the following questions:\n",
    "\n",
    "**a) What do you notice about the effect of hue data on classifiction accuracy? Do you notice anything strange, or anything that was expected?**\n",
    "\n",
    "**b) Why do you think that hue was a good feature to include, especially with these data sets?**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Please enter your answer in the following box.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUT ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Congratulations, you've completed this problem set! Please save this notebook, validate it, and submit it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
